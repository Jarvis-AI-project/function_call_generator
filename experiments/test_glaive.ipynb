{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "import torch\n",
    "\n",
    "PATH = \"/mnt/nvme/MODELS/LLM/glaive-function-calling-v2-small/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    PATH, \n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "SYSTEM: You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed-\n",
    "[\n",
    "  {\n",
    "    \"name\": \"tv.is_on\",\n",
    "    \"description\": \"Check if the TV is on. Returns True if the TV is on, False otherwise\",\n",
    "    \"parameters\": [\n",
    "      \"name\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"name of TV to check\"\n",
    "      }\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"tv.get\",\n",
    "    \"description\": \"Get the list of TVs\",\n",
    "    \"parameters\": [],\n",
    "    \"returns\": \"list of TVs\"\n",
    "  }\n",
    "]\n",
    "USER: Hi, I am trying to watch TV but it is not working.\n",
    "ASSISTANT: Sure, I can help with that. Do you know the name of the TV?\n",
    "USER: Yes, its called \"Living Room TV\"\n",
    "ASSISTANT: <functioncall> {\"name\": \"tv.is_on\", \"arguments\": '{\"name\": \"Living Room TV\"}'}\n",
    "FUNCTION: {\"tv.is_on\": None}\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# outputs = model.generate(**inputs,do_sample=True,temperature=0.1,top_p=0.95,max_new_tokens=100)\n",
    "outputs = model.generate(**inputs,do_sample=False,max_new_tokens=512)\n",
    "\n",
    "print(tokenizer.decode(outputs[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "SYSTEM: You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed-\n",
    "[\n",
    "    {\n",
    "        \"name\": \"cab.ride\",\n",
    "        \"description\": \"Find suitable ride for customers given the location, type of ride, and the amount of time the customer is willing to wait as parameters.\",\n",
    "        \"parameters\":  {\n",
    "            \"start_loc\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"location of the starting place of the uber ride\"\n",
    "            },\n",
    "            \"end_loc\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"location of the ending place of the uber ride\"\n",
    "            },\n",
    "            \"type\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"normal\", \"plus\", \"premium\"],\n",
    "                \"default\": \"normal\",\n",
    "                \"description\": \"types of uber ride user is ordering\"\n",
    "            },\n",
    "            \"time\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"the amount of time in minutes the customer is willing to wait in minutes\"\n",
    "            },\n",
    "            \"platform\": {\n",
    "                \"type\": \"string\",\n",
    "                \"default\": \"uber\",\n",
    "                \"enum\": [\"uber\", \"ola\"],\n",
    "                \"description\": \"the platform the user is ordering the ride from\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"start_loc\", \"end_loc\", \"type\", \"time\", \"platform\"]\n",
    "    }\n",
    "]\n",
    "USER: call an cab to nodia sector 62 in twenty two minutes using ola.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# outputs = model.generate(**inputs,do_sample=True,temperature=0.7,top_p=0.95,max_new_tokens=512)\n",
    "outputs = model.generate(**inputs,do_sample=False,max_new_tokens=512)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.encode(\"\"\"{\"name\": \"cab.ride\", \"arguments\": '{\"start_loc\": \"nodia\",\"end_loc\": \"nodia\",\"type\": \"normal\",\"time\": 120,\"platform\": \"ola\"}'}\"\"\", return_tensors=\"pt\")[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "function_call_generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
