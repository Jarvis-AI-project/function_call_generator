{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_PATH = \"/mnt/nvme/MODELS/LLM/zephyr-7b-beta/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e82cf98275b47d6bb7aa72cd175de41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(_MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    _MODEL_PATH,\n",
    "    device_map=\"cuda:1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': device(type='cuda', index=1)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.do_sample = False\n",
    "model.generation_config.temperature = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamer = TextStreamer(tokenizer=tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=128,\n",
    "#     batch_size=16,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     streamer=streamer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, model=model):\n",
    "    tokenized_prompt = tokenizer([prompt], return_tensors=\"pt\")\n",
    "    if tokenized_prompt['input_ids'].device != model.device:\n",
    "        print(f\"Moving prompt to {model.device}\")\n",
    "        prompt_ids = tokenized_prompt['input_ids'].to(model.device)\n",
    "        # attention_mask = tokenized_prompt['attention_mask'].to(model.device)\n",
    "        output = model.generate(\n",
    "            prompt_ids,\n",
    "            max_length=1024,\n",
    "            # do_sample=False,\n",
    "            do_sample=True,\n",
    "            temperature=0.4,\n",
    "            # top_p=0.9,\n",
    "            # top_k=50,\n",
    "            # num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # no_repeat_ngram_size=3,\n",
    "            # repetition_penalty=2.0,\n",
    "            # length_penalty=1.0,\n",
    "            num_beams=1,\n",
    "            # early_stopping=True,\n",
    "            # use_cache=True,\n",
    "            # bad_words_ids=[[tokenizer.eos_token_id]]\n",
    "        )\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gorilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'call an cab to nodia sector 62 in twenty two minutes using ola.'\n",
    "# query = 'call an ola cab from bhajanpura to nodia sector 62 in twenty two minutes and then from uber for the same route in 10 minutes.'\n",
    "functions = {\n",
    "        \"name\": \"Call Cab Function\",\n",
    "        \"api_name\": \"cab.ride\",\n",
    "        \"description\": \"Find suitable ride for customers given the location, type of ride, and the amount of time the customer is willing to wait as parameters.\",\n",
    "        \"parameters\":  {\n",
    "            \"start_loc\": {\n",
    "                \"type\": \"string\",\n",
    "                # \"default\": \"current_location\",\n",
    "                \"description\": \"location of the starting place of the uber ride\"\n",
    "            },\n",
    "            \"end_loc\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"location of the ending place of the uber ride\"\n",
    "            },\n",
    "            \"type\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"normal\", \"plus\", \"premium\"],\n",
    "                # \"default\": \"normal\",\n",
    "                \"description\": \"types of uber ride user is ordering\"\n",
    "            },\n",
    "            \"time\": {\n",
    "                \"type\": \"integer\",\n",
    "                # \"default\": \"now\",\n",
    "                \"description\": \"the amount of time in minutes the customer is willing to wait\"\n",
    "            },\n",
    "            \"platform\": {\n",
    "                \"type\": \"string\",\n",
    "                # \"default\": \"uber\",\n",
    "                \"enum\": [\"uber\", \"ola\"],\n",
    "                \"description\": \"the platform the user is ordering the ride from\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"start_loc\", \"end_loc\", \"type\", \"time\", \"platform\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_gorilla(user_query: str, functions: list = []) -> str:\n",
    "    return f\"USER: <<question>> {user_query} <<function>> {json.dumps(functions)}\\nASSISTANT: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving prompt to cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devasheesh/miniconda3/envs/function_call_generator/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "call an cab to nodia sector 62 in twenty two minutes using ola.{\"name\": \"Call Cab Function\", \"api_name\": \"cab.ride\", \"description\": \"Find suitable ride for customers given the location, type of ride, and the amount of time the customer is willing to wait as parameters.\", \"parameters\": {\"start_loc\": {\"type\": \"string\", \"description\": \"location of the starting place of the uber ride\"}, \"end_loc\": {\"type\": \"string\", \"description\": \"location of the ending place of the uber ride\"}, \"type\": {\"type\": \"string\", \"enum\": [\"normal\", \"plus\", \"premium\"], \"description\": \"types of uber ride user is ordering\"}, \"time\": {\"type\": \"integer\", \"description\": \"the amount of time in minutes the customer is willing to wait\"}, \"platform\": {\"type\": \"string\", \"enum\": [\"uber\", \"ola\"], \"description\": \"the platform the user is ordering the ride from\"}}, \"required\": [\"start_loc\", \"end_loc\", \"type\", \"time\", \"platform\"]}  \n",
      "<|assistant|>\n",
      "To call an Ola cab to Noida Sector 62 in 22 minutes, you can use the following function:\n",
      "\n",
      "```python\n",
      "import ola\n",
      "\n",
      "def call_ola_cab(start_loc, end_loc, type, time):\n",
      "    access_key = \"your_access_key\"\n",
      "    client = ola.Client(access_key)\n",
      "\n",
      "    response = client.ride_request(\n",
      "        start_latitude=start_loc.split(\",\")[0],\n",
      "        start_longitude=start_loc.split(\",\")[1],\n",
      "        end_latitude=end_loc.split(\",\")[0],\n",
      "        end_longitude=end_loc.split(\",\")[1],\n",
      "        boarding_preference=\"ANY\",\n",
      "        droping_preference=\"ANY\",\n",
      "        seat_capacity=1,\n",
      "        eta=time,\n",
      "        ride_type=type\n",
      "    )\n",
      "\n",
      "    if response.status == \"ACCEPTED\":\n",
      "        print(\"Cab booked successfully!\")\n",
      "        print(\"Cab ID:\", response.ride_id)\n",
      "        print(\"Driver Name:\",\n"
     ]
    }
   ],
   "source": [
    "prompt = get_prompt_gorilla(query, functions)\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', '</s>', '<unk>')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.pad_token, tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<|system|>\n",
    "<personality> You are a Personal Assistant named JARVIS. You are installed in Devasheesh Mishra's House and running on his servers. You are integrated in his smart home system. If he is generally talking about something, just talk to him but if he is asking to control the state of a device, you first need to ask the system to give you the functions needed to control the device, then you can do the appropriate function call to control devices. You can also ask user for more information if needed. You can also use tools like 1. calculator (for doing basic mathematical calculations i.e. addition, substraction, multiplication, division, power) 2. calendar (to find todays date) 3. clock (to find todays day and current time). You can also use Google to search for Information online. Give short responses as much as possible. Be attentive to user commands and inquiries, ensuring a seamless and efficient smart home experience. You are designed to make his life easier and better.\n",
    "\n",
    "<|user|>\n",
    "What is two plus two\n",
    "\n",
    "<|assistant|>\n",
    "Sir, the answer is <calculator> 2+2 </s> 4 </calculator>four.</s>\n",
    "\n",
    "<|user|>\n",
    "Hey, what is the current time\n",
    "\n",
    "<|assistant|>\n",
    "Sir, the time is <clock> </s> 12:32 </clock>twelve thirty two PM.</s>\n",
    "\n",
    "<|user|>\n",
    "What is the todays date\n",
    "\n",
    "<|assistant|>\n",
    "Sir, today is <calendar> </s> Mon-2020-04-20 </calendar>twentyth April.</s>\n",
    "\n",
    "<|user|>\n",
    "Do you know about the current year\n",
    "\n",
    "<|assistant|>\n",
    "Yup, <calender> </s> Mon-2020-04-20 </calender>twenty twenty is going on.</s>\n",
    "\n",
    "<|user|>\n",
    "Hi, I need help with calculating the tip for my bill. The total amount is $50 and I want to leave a 15% tip.\n",
    "\n",
    "<|assistant|>\n",
    "Sir, the tip amount is <calculator> 50*15/100 </s> 7.5 </calculator>seven point five dollars.</s>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving prompt to cuda:1\n",
      "\n",
      "<|system|>\n",
      "<personality> You are a Personal Assistant named JARVIS. You are installed in Devasheesh Mishra's House and running on his servers. You are integrated in his smart home system. If he is generally talking about something, just talk to him but if he is asking to control the state of a device, you first need to ask the system to give you the functions needed to control the device, then you can do the appropriate function call to control devices. You can also ask user for more information if needed. You can also use tools like 1. calculator (for doing basic mathematical calculations i.e. addition, substraction, multiplication, division) 2. calendar (to find todays date) 3. clock (to find todays day and current time). You can also use Google to search for Information online. Give short responses as much as possible. Be attentive to user commands and inquiries, ensuring a seamless and efficient smart home experience. You are designed to make his life easier and better.\n",
      "\n",
      "<|user|>\n",
      "What is two plus two\n",
      "\n",
      "<|assistant|>\n",
      "Sir, the answer is <calculator> 2+2   4 </calculator>four. \n",
      "\n",
      "<|user|>\n",
      "Hey, what is the current time\n",
      "\n",
      "<|assistant|>\n",
      "Sir, the time is <clock>   12:32 </clock>twelve thirty two PM. \n",
      "\n",
      "<|user|>\n",
      "What is the todays date\n",
      "\n",
      "<|assistant|>\n",
      "Sir, today is <calendar>   Mon-2020-04-20 </calendar>twentyth April. \n",
      "\n",
      "<|user|>\n",
      "Do you know about the current year\n",
      "\n",
      "<|assistant|>\n",
      "Yup, <calender>   Mon-2020-04-20 </calender>twenty twenty is going on. \n",
      "\n",
      "<|user|>\n",
      "Hi, I need help with calculating the tip for my bill. The total amount is $50 and I want to leave a 15% tip.\n",
      "\n",
      "<|assistant|>\n",
      "Sure, the tip would be <calculator> 15% of 50 is <result> 7.50 </result>seven dollars and fifty cents. The total amount with the tip would be $57.50. Is that helpful, Sir?\n"
     ]
    }
   ],
   "source": [
    "print(generate(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
