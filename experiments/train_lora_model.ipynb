{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    "    GenerationConfig,\n",
    "    logging,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    pipeline,\n",
    ")\n",
    "from transformers.trainer_callback import (\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")\n",
    "from transformers.trainer_utils import (\n",
    "    IntervalStrategy,\n",
    ")\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, random_split\n",
    "from pymilvus import Collection, db, connections\n",
    "import torch\n",
    "import datasets\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdevasheeshmishra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"datasets/synthetic-mt/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _BASE_MODEL_PATH = Path('../models/zephyr-7b-beta/')\n",
    "_BASE_MODEL_PATH = Path('../models/Mistral-7B-Instruct-v0.2/')\n",
    "# _BASE_MODEL_PATH = Path('../models/phi-1_5/')\n",
    "# _BASE_MODEL_PATH = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "_LORA_OUTPUT_PATH = Path('output/loras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    _BASE_MODEL_PATH,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False\n",
    ")\n",
    "# model_config = AutoConfig.from_pretrained(_BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/Mistral-7B-Instruct-v0.2/config.json\n",
      "Model config MistralConfig {\n",
      "  \"_name_or_path\": \"../models/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ../models/Mistral-7B-Instruct-v0.2/pytorch_model.bin.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e948780a0a481c962e06e30591face",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at ../models/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file ../models/Mistral-7B-Instruct-v0.2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    _BASE_MODEL_PATH,\n",
    "    # config=base_model_config,\n",
    "    device_map='auto',\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:  # If more than 1 GPU\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    setattr(model, 'is_model_parallel', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "for param in model.parameters():\n",
    "    # Turning off gradient calculation for base model as we want to train lora, not base model\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<s>': 1}, {'</s>': 2}, {None: None}, {'<unk>': 0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{tokenizer.bos_token: tokenizer.bos_token_id}, \\\n",
    "{tokenizer.eos_token: tokenizer.eos_token_id}, \\\n",
    "{tokenizer.pad_token: tokenizer.pad_token_id}, \\\n",
    "{tokenizer.unk_token: tokenizer.unk_token_id},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.do_sample = False\n",
    "model.generation_config.do_sample = False\n",
    "# model.config.temperature=0.0\n",
    "# model.generation_config.temperature=0.0\n",
    "\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32003. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: <calculator>, ID: [1, 32000, 2]\n",
      "Token: </calculator>, ID: [1, 32001, 2]\n",
      "Token: <stop>, ID: [1, 32002, 2]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = tokenizer.get_vocab().keys()\n",
    "new_tokens = [\"<calculator>\", \"</calculator>\", \"<stop>\"]\n",
    "for token in new_tokens:\n",
    "    if token not in vocabulary:\n",
    "        tokenizer.add_tokens(token)\n",
    "        print(f\"Token: {token}, ID: {tokenizer.encode(token)}\")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPER PARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"You are a Personal Assistant named JARVIS. You are installed in Devasheesh Mishra's House and running on his servers. You are integrated in his smart home system. If he is generally talking about something, just talk to him but if he is asking to control the state of a device, you first need to ask the system to give you the functions needed to control the device, then you can do the appropriate function call to control devices. You can also ask user for more information if needed. You can also use tools like 1. calculator (for doing basic mathematical calculations i.e. addition, substraction, multiplication, division) 2. calendar (to find todays date) 3. clock (to find todays day and current time). You can also use Google to search for Information online. Give short responses as much as possible. Be attentive to user commands and inquiries, ensuring a seamless and efficient smart home experience. You are designed to make his life easier and better.\"\n",
    "SYSTEM_PROMPT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_DEVICE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "EPOCHS = 10\n",
    "WARMUP_STEPS = 0\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Zephyr Model Prompt Template:\n",
    "\n",
    "```text\n",
    "<|system|>\n",
    "You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "<|user|>\n",
    "How many helicopters can a human eat in one sitting?</s>\n",
    "<|assistant|>\n",
    "Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        connections.connect(\n",
    "            alias=\"default\",\n",
    "            host=os.getenv(\"MILVUS_HOST\"),\n",
    "            port=19530,\n",
    "            user=os.getenv(\"MILVUS_USER\"),\n",
    "            password=os.getenv(\"MILVUS_PASSWORD\"),\n",
    "        )\n",
    "        db.using_database(\"JARVIS\")\n",
    "        self.collection = Collection(\"calculator\")\n",
    "        self.data_itrator = self.collection.query(\n",
    "            # batch_size=1,\n",
    "            output_fields=[\"conversation\"],\n",
    "            expr=\"conversation_id > 0\",\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.collection.num_entities\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # conversation: dict = self.data_itrator.next()[0]\n",
    "        conversation: dict = self.data_itrator[idx]\n",
    "        conversation = self.apply_prompt_template(conversation)\n",
    "        # return conversation\n",
    "        return {\n",
    "                'input_ids': conversation['input_ids'].squeeze(),\n",
    "                'attention_mask': conversation['attention_mask'].squeeze(),\n",
    "            }\n",
    "        # return {\n",
    "        #         'input_ids': torch.tensor(conversation['input_ids'], device=base_model.device),\n",
    "        #         'attention_mask': torch.tensor(conversation['attention_mask'], device=base_model.device)\n",
    "        #     }\n",
    "\n",
    "    def apply_prompt_template(self, conversation: dict):\n",
    "        # add </s> at end of lines which do not end with </s>\n",
    "        # conversation[\"conversation\"][\"data\"] = ''.join([x + '</s>\\n' if not x.strip().endswith(\"</s>\") else x + '\\n' for x in conversation[\"conversation\"][\"data\"].split(\"\\n\")]).strip()\n",
    "\n",
    "        # apply zephyr-7b-beta chat template\n",
    "        prompt = f\"<|system|>\\n{SYSTEM_PROMPT}\\n{conversation[\"conversation\"][\"data\"].replace(\"USER: \", \"<|user|>\\n\").replace(\"ASSISTANT: \", \"<|assistant|>\\n\")}\" \\\n",
    "            .replace('</s>', '<stop>').replace('  ', ' ')\n",
    "\n",
    "        # return prompt\n",
    "        # return tokenizer(prompt, return_tensors=\"pt\", padding=True).to(base_model.device)\n",
    "        return tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Gradient Accumulation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-accumulation)\n",
    "\n",
    "    The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model’s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU’s memory. In turn, however, the added forward and backward passes can slow down the training a bit.\n",
    "\n",
    "### [Gradient Checkpointing](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing)\n",
    "\n",
    "    Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\n",
    "\n",
    "    Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See this great article explaining the ideas behind gradient checkpointing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.gradient_checkpointing_enable()\n",
    "# base_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,262,976 || all params: 7,269,019,648 || trainable%: 0.37505712352147985\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    # peft_type: str | PeftType = None,\n",
    "    # auto_mapping: dict | None = None,\n",
    "    base_model_name_or_path=_BASE_MODEL_PATH.name,\n",
    "    # revision: str = None,\n",
    "    # task_type = TaskType.CAUSAL_LM,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode: bool = False,\n",
    "    r=32,  # ! 8, 16, 32, 64\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=32,  # ! 8, 16, 32\n",
    "    lora_dropout=0.02,\n",
    "    # fan_in_fan_out: bool = False,\n",
    "    bias=\"none\",\n",
    "    # modules_to_save: List[str] | None = None,\n",
    "    # init_lora_weights: bool = True,\n",
    "    # layers_to_transform: List[int] | int | None = None,\n",
    "    # layers_pattern: str | None = None\n",
    ")\n",
    "model = get_peft_model(model, lora_config, adapter_name='jarvis-calculator-v0_1')\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parm in model.named_parameters():\n",
    "#     print(parm[0] + '\\n' if parm[1].requires_grad else '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MilvusDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_size = len(dataset)\n",
    "# train_size = int(0.95 * total_size)\n",
    "# test_size = total_size - train_size\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "# print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "# train_dataset, test_dataset = list(train_dataset), list(test_dataset)\n",
    "\n",
    "# train_dataset, test_dataset = [{\"text\": text} for text in train_dataset], [{\"text\": text} for text in test_dataset]\n",
    "\n",
    "# train_dataset, test_dataset = datasets.Dataset.from_list(train_dataset), datasets.Dataset.from_list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(train_dataset[4]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you ran this function, you will need to rerun the cell above to get the train_dataset and test_dataset\n",
    "# def plot_data_lengths(dataset):\n",
    "#     lengths = [len(x['input_ids']) for x in dataset]\n",
    "#     print(len(lengths))\n",
    "\n",
    "#     # Plotting the histogram\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.hist(lengths, bins=40, alpha=0.7, color='blue')\n",
    "#     plt.xlabel('Length of input_ids')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.title('Distribution of Lengths of input_ids')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_data_lengths(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "run_name = \"calculator-lora\"\n",
    "training_args = TrainingArguments(\n",
    "    #! Training\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    # gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_DEVICE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "\n",
    "    #! Datatypes\n",
    "    fp16=True if model.parameters().__next__().dtype == torch.float16 else False,\n",
    "    fp16_full_eval=True if model.parameters().__next__().dtype == torch.float16 else False,\n",
    "    bf16=True if model.parameters().__next__().dtype == torch.bfloat16 else False,\n",
    "    bf16_full_eval=True if model.parameters().__next__().dtype == torch.bfloat16 else False,\n",
    "\n",
    "    #! Evaluation\n",
    "    # evaluation_strategy=IntervalStrategy.NO,\n",
    "    # eval_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n",
    "    metric_for_best_model=\"train_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "\n",
    "    #! Logging\n",
    "    logging_strategy=IntervalStrategy.STEPS,\n",
    "    logging_steps=len(dataset) // (BATCH_SIZE_PER_DEVICE * GRADIENT_ACCUMULATION_STEPS * EPOCHS),\n",
    "\n",
    "    #! Checkpointing\n",
    "    save_strategy=IntervalStrategy.STEPS,\n",
    "    save_steps=10,\n",
    "    output_dir=_LORA_OUTPUT_PATH,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTrainer(Trainer):\n",
    "#     # def __init__(self, *args, **kwargs):\n",
    "#     #     super().__init__(*args, **kwargs)\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         return super().compute_loss(model, inputs, return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "# wandb.init(project=\"jarvis-calculator-lora\", name=run_name, config=training_args)\n",
    "# trainer = CustomTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     # data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "#     tokenizer=tokenizer,\n",
    "#     train_dataset=dataset,\n",
    "#     # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_test = dataset[0]\n",
    "# sample_test = tokenizer.decode(sample_test['input_ids'], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculatorErrorCallback(TrainerCallback):\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        model = state.model\n",
    "        tokenizer = state.tokenizer\n",
    "        if not model.training:\n",
    "            raise Exception(\"Model is not in training mode\")\n",
    "        \n",
    "        # return super().on_step_end(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/nvme/JARVIS/function_call_generator/experiments/wandb/run-20240121_131353-0so15bl1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devasheeshmishra/jarvis-calculator-lora/runs/0so15bl1' target=\"_blank\">calculator-lora</a></strong> to <a href='https://wandb.ai/devasheeshmishra/jarvis-calculator-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devasheeshmishra/jarvis-calculator-lora' target=\"_blank\">https://wandb.ai/devasheeshmishra/jarvis-calculator-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devasheeshmishra/jarvis-calculator-lora/runs/0so15bl1' target=\"_blank\">https://wandb.ai/devasheeshmishra/jarvis-calculator-lora/runs/0so15bl1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "***** Running training *****\n",
      "  Num examples = 54\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 540\n",
      "  Number of trainable parameters = 27,262,976\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 12:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.866800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.634900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.573100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.638600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.414500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.310100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output/loras/tmp-checkpoint-10\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-10/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-10/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-20\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-20/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-20/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-30\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-30/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-30/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-40\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-40/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-40/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-50\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-50/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-50/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-60\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-60/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-60/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-70\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-70/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-70/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-70/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-80\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-80/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-80/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-90\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-90/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-90/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-100\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-100/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-100/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-110\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-110/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-110/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-110/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-120\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-120/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-120/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-130\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-130/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-130/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-130/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-140\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-140/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-140/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-140/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-150\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-150/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-150/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-160\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-160/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-160/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-170\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-170/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-170/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-180\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-180/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-180/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-190\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-190/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-190/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-200\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-200/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-200/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-210\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-210/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-210/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-210/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-220\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-220/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-220/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-220/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-230\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-230/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-230/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-230/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-240\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-240/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-240/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-250\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-250/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-250/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-260\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-260/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-260/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-260/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-270\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-270/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-270/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-270/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-280\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-280/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-280/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-280/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-290\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-290/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-290/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-290/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-300\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-300/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-300/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-310\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-310/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-310/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-310/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-320\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-320/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-320/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-330\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-330/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-330/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-330/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-340\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-340/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-340/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-350\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-350/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-350/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-360\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-360/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-360/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-360/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-370\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-370/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-370/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-370/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-380\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-380/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-380/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-390\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-390/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-390/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-390/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-400\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-400/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-400/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-410\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-410/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-410/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-410/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-420\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-420/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-420/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-420/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-430\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-430/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-430/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-430/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-440\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-440/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-440/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-440/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-450\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-450/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-450/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-460\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-460/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-460/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-460/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-470\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-470/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-470/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-470/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-480\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-480/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-480/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-480/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-490\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-490/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-490/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-490/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-500\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-500/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-500/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-510\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-510/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-510/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-520\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-520/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-520/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-520/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-530\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-530/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-530/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-530/added_tokens.json\n",
      "Saving model checkpoint to output/loras/tmp-checkpoint-540\n",
      "tokenizer config file saved in output/loras/tmp-checkpoint-540/tokenizer_config.json\n",
      "Special tokens file saved in output/loras/tmp-checkpoint-540/special_tokens_map.json\n",
      "added tokens file saved in output/loras/tmp-checkpoint-540/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=540, training_loss=0.26137542092689764, metrics={'train_runtime': 722.2141, 'train_samples_per_second': 0.748, 'train_steps_per_second': 0.748, 'total_flos': 5920489104998400.0, 'train_loss': 0.26137542092689764, 'epoch': 10.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "wandb.init(project=\"jarvis-calculator-lora\", name=run_name, config=training_args)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[\n",
    "        # EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        # CalculatorErrorCallback(),\n",
    "    ],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"jarvis-calculator-lora\", name=run_name, config=training_args)\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=dataset,\n",
    "#     dataset_text_field='text',\n",
    "#     peft_config=lora_config,\n",
    "#     args=training_args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     packing=False,\n",
    "#     max_seq_length=MAX_LENGTH,\n",
    "#     # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_bos_token = False\n",
    "tokenizer.add_eos_token = False\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer(dataset['text'][0])['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<|system|>\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m<|user|>\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mHey, I am trying to calculate the total cost of 10 items that cost 5 rupees each. Can you help me?\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m<|assistant|>\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"<|system|>\n",
    "<|user|>\n",
    "Hey, I am trying to calculate the total cost of 10 items that cost 5 rupees each. Can you help me?\n",
    "<|assistant|>\n",
    "Of course! Please provide me with the cost of each item and the number of items.  <stop>\n",
    "<|user|>\n",
    "There are 10 items in total.\n",
    "<|assistant|>\n",
    "Great! And the cost of each item is five rupees?   <stop>\n",
    "<|user|>\n",
    "No, the cost of each item is 130 rupees.\n",
    "<|assistant|>\n",
    "I see. So, the total cost of all the items would be  <calculator> 10*130    <stop>\n",
    "<|user|>\n",
    "what is the total height of the chair including its legs ?\n",
    "<|assistant|>\n",
    "To find the total height of the chair including its legs, I'll need to know the height of the chair's seat and the length of each leg.   <stop>\n",
    "<|user|>\n",
    "the chair height is two and a half feet without legs and each leg is half a foot tall.\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "prompt = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stops: list = []):\n",
    "        StoppingCriteria.__init__(self)\n",
    "        self.stops = stops\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for token in self.stops:\n",
    "            if input_ids[0][-1].cpu().numpy() == token:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    **prompt,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    # temperature=0.6,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    stopping_criteria=StoppingCriteriaList([\n",
    "        CustomStoppingCriteria(stops=[tokenizer.encode(\"<stop>\", add_special_tokens=False)[-1]])\n",
    "    ]),\n",
    ")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.LongTensor([2287]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
