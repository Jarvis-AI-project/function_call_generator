{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TextStreamer, \n",
    "    GenerationConfig, \n",
    "    logging,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import datasets\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pymilvus import Collection, db, connections\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "import os\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdevasheeshmishra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"datasets/synthetic-mt/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BASE_MODEL_PATH = Path('/mnt/nvme/MODELS/LLM/zephyr-7b-beta/')\n",
    "_LORA_OUTPUT_PATH = Path('output/loras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading configuration file /mnt/nvme/MODELS/LLM/zephyr-7b-beta/config.json\n",
      "Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/mnt/nvme/MODELS/LLM/zephyr-7b-beta\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model_tokenizer = AutoTokenizer.from_pretrained(_BASE_MODEL_PATH, use_fast=False)\n",
    "base_model_config = AutoConfig.from_pretrained(_BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file /mnt/nvme/MODELS/LLM/zephyr-7b-beta/pytorch_model.bin.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbc2f27851943b691b028a1a4ec4bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at /mnt/nvme/MODELS/LLM/zephyr-7b-beta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file /mnt/nvme/MODELS/LLM/zephyr-7b-beta/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    _BASE_MODEL_PATH,\n",
    "    config=base_model_config, \n",
    "    device_map='cuda:1', \n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in base_model.parameters():\n",
    "    # Turning off gradient calculation for base model as we want to train lora, not base model\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>', '</s>', '<unk>')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_tokenizer.bos_token, base_model_tokenizer.pad_token, base_model_tokenizer.eos_token, base_model_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a Personal Assistant named JARVIS. You are installed in Devasheesh Mishra's House and running on his servers. You are integrated in his smart home system. If he is generally talking about something, just talk to him but if he is asking to control the state of a device, you first need to ask the system to give you the functions needed to control the device, then you can do the appropriate function call to control devices. You can also ask user for more information if needed. You can also use tools like 1. calculator (for doing basic mathematical calculations i.e. addition, substraction, multiplication, division) 2. calendar (to find todays date) 3. clock (to find todays day and current time). You can also use Google to search for Information online. Give short responses as much as possible. Be attentive to user commands and inquiries, ensuring a seamless and efficient smart home experience. You are designed to make his life easier and better.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 2\n",
    "WARMUP_STEPS = 100\n",
    "TOTAL_STEPS = 1000\n",
    "LEARNING_RATE = 4e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Zephyr Model Prompt Template:\n",
    "```text\n",
    "<|system|>\n",
    "You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "<|user|>\n",
    "How many helicopters can a human eat in one sitting?</s>\n",
    "<|assistant|>\n",
    "Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        connections.connect(\n",
    "            alias=\"default\",\n",
    "            host = os.getenv(\"MILVUS_HOST\"),\n",
    "            port = 19530,\n",
    "            user=os.getenv(\"MILVUS_USER\"),\n",
    "            password=os.getenv(\"MILVUS_PASSWORD\"),\n",
    "        )\n",
    "        db.using_database(\"JARVIS\")\n",
    "        self.collection = Collection(\"calculator\")\n",
    "        self.data_itrator = self.collection.query_iterator(\n",
    "                                batch_size=1,\n",
    "                                output_fields=[\"conversation\"],\n",
    "                                expr=\"conversation_id > 0\",\n",
    "                            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.collection.num_entities\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation: dict = self.data_itrator.next()[0]\n",
    "        conversation = self.apply_prompt_template(conversation)\n",
    "        return conversation\n",
    "        # return {\n",
    "        #         'input_ids': torch.tensor(conversation['input_ids'], device=base_model.device), \n",
    "        #         'attention_mask': torch.tensor(conversation['attention_mask'], device=base_model.device)\n",
    "        #     }\n",
    "\n",
    "    def apply_prompt_template(self, conversation: dict):\n",
    "        # add </s> at end of lines which do not end with </s>\n",
    "        conversation[\"conversation\"][\"data\"] = ''.join([x + '</s>\\n' if not x.strip().endswith(\"</s>\") else x + '\\n' for x in conversation[\"conversation\"][\"data\"].split(\"\\n\")])\n",
    "\n",
    "        # apply zephyr-7b-beta chat template\n",
    "        prompt = f\"<|system|>\\n{SYSTEM_PROMPT}</s>\\n{conversation[\"conversation\"][\"data\"].replace(\"USER: \", \"<|user|>\\n\").replace(\"ASSISTANT: \", \"<|assistant|>\\n\")}\"\n",
    "        # return prompt\n",
    "        return base_model_tokenizer(prompt, return_tensors=\"pt\", padding=True).to(base_model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Gradient Accumulation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-accumulation)\n",
    "    The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model’s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU’s memory. In turn, however, the added forward and backward passes can slow down the training a bit.\n",
    "\n",
    "### [Gradient Checkpointing](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing)\n",
    "    Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\n",
    "\n",
    "    Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See this great article explaining the ideas behind gradient checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.gradient_checkpointing_enable()\n",
    "base_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_trainable_parameters(model: nn.Module):\n",
    "#     \"\"\"\n",
    "#     Prints the number of trainable parameters in the model.\n",
    "#     \"\"\"\n",
    "#     trainable_params = 0\n",
    "#     all_param = 0\n",
    "#     for name, param in model.named_parameters():\n",
    "#         all_param += param.numel()\n",
    "#         if param.requires_grad:\n",
    "#             trainable_params += param.numel()\n",
    "#     print(\n",
    "#         f\"trainable params: {trainable_params} || all params: {all_param} || trainable(%): {100 * trainable_params / all_param}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@manyi.yim/more-about-loraconfig-from-peft-581cf54643db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940290959023318\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    # peft_type: str | PeftType = None,\n",
    "    # auto_mapping: dict | None = None,\n",
    "    # base_model_name_or_path: str = None,\n",
    "    # revision: str = None,\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    "    # inference_mode: bool = False,\n",
    "    r = 16, #! 8, 16, 32, 64\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha = 32, #! 8, 16, 32\n",
    "    lora_dropout = 0.05, #! 0.05\n",
    "    # fan_in_fan_out: bool = False,\n",
    "    bias = \"none\",\n",
    "    # modules_to_save: List[str] | None = None,\n",
    "    # init_lora_weights: bool = True,\n",
    "    # layers_to_transform: List[int] | int | None = None,\n",
    "    # layers_pattern: str | None = None\n",
    ")\n",
    "peft_model = get_peft_model(base_model, lora_config, adapter_name='jarvis-calculator-v0_1')\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "trainig_parms = TrainingArguments(\n",
    "    # gradient_checkpointing=True,\n",
    "    # gradient_checkpointing_kwargs={\"use_reentrant\":False},\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    \n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=TOTAL_STEPS,\n",
    "\n",
    "    # optim=\"adamw_torch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    output_dir=_LORA_OUTPUT_PATH,\n",
    "    \n",
    "    report_to=\"wandb\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "\n",
    "    eval_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    save_steps=10,\n",
    "    \n",
    "    save_total_limit=2,\n",
    "\n",
    "    # save_only_model=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MilvusDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [next(iter(data)) for _ in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:int(len(dataset) * 0.8)]\n",
    "eval_dataset = dataset[int(len(dataset) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = transformers.EarlyStoppingCallback(10, 1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=trainig_parms,\n",
    "    tokenizer=base_model_tokenizer,\n",
    "    callbacks=[early_stop],\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(base_model_tokenizer, mlm=False),\n",
    "    # data_collator=lambda data: {\n",
    "    #     'input_ids': torch.stack([item['input_ids'] for item in data]),\n",
    "    #     'attention_mask': torch.stack([item['attention_mask'] for item in data])\n",
    "    # },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('birgermoell/open_assistant_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_tokenizer.add_eos_token = True\n",
    "base_model_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cleaning and preprocessing functions.\n",
    "def text_to_dialogue(text):\n",
    "    return [sentence.replace('User:', '').replace('Chip:', '').strip() for sentence in text.split('Assistant:')]\n",
    "\n",
    "def dialogue_to_chat(dialogue):\n",
    "    out = [{'role': 'system', 'content': 'You are a friendly chatbot assistant.'}]\n",
    "    for idx, message in enumerate(dialogue):\n",
    "        role = 'user' if idx%2==0 else 'assistant'\n",
    "        out.append({'role': role, 'content': message})\n",
    "    return out\n",
    "\n",
    "def chat_to_input(chat):\n",
    "    return base_model_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "def process_example(example):\n",
    "    out = text_to_dialogue(example)             # Clean up data from redundant words\n",
    "    out = dialogue_to_chat(out)                 # Convert to ChatML format\n",
    "    out = chat_to_input(out)                    # Add a task description & Apply base model chat template\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply on dataset.\n",
    "data = list(map(process_example, df['text']))\n",
    "\n",
    "# Shuffle dataset.\n",
    "from random import shuffle\n",
    "shuffle(data)\n",
    "\n",
    "# Tokenize data.\n",
    "tokenized_data = list(map(base_model_tokenizer, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(.99 * len(data))\n",
    "train_data, val_data = tokenized_data[:split_idx], tokenized_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model_tokenizer.decode(dataset[0]['input_ids'])), print(base_model_tokenizer.decode(train_data[811]['input_ids']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
