{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    "    GenerationConfig,\n",
    "    logging,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    pipeline,\n",
    ")\n",
    "from transformers.trainer_callback import (\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, random_split\n",
    "from pymilvus import Collection, db, connections\n",
    "import torch\n",
    "import datasets\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdevasheeshmishra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"datasets/synthetic-mt/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BASE_MODEL_PATH = Path('../models/zephyr-7b-beta/')\n",
    "# _BASE_MODEL_PATH = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "_LORA_OUTPUT_PATH = Path('output/loras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    _BASE_MODEL_PATH,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False\n",
    ")\n",
    "# model_config = AutoConfig.from_pretrained(_BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/zephyr-7b-beta/config.json\n",
      "Model config MistralConfig {\n",
      "  \"_name_or_path\": \"../models/zephyr-7b-beta\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ../models/zephyr-7b-beta/pytorch_model.bin.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67863bf123eb43c880442ebb1f42d536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at ../models/zephyr-7b-beta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file ../models/zephyr-7b-beta/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    _BASE_MODEL_PATH,\n",
    "    # config=base_model_config,\n",
    "    device_map='auto',\n",
    "    # torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:  # If more than 1 GPU\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model.is_model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "for param in model.parameters():\n",
    "    # Turning off gradient calculation for base model as we want to train lora, not base model\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<s>': 1}, {'</s>': 2}, {'</s>': 2}, {'<unk>': 0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{tokenizer.bos_token: tokenizer.bos_token_id}, \\\n",
    "    {tokenizer.eos_token: tokenizer.eos_token_id}, \\\n",
    "    {tokenizer.pad_token: tokenizer.pad_token_id}, \\\n",
    "    {tokenizer.unk_token: tokenizer.unk_token_id},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.do_sample = False\n",
    "model.generation_config.do_sample = False\n",
    "# model.config.temperature=0.0\n",
    "# model.generation_config.temperature=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32003. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: <calculator>, ID: [1, 32000, 2]\n",
      "Token: </calculator>, ID: [1, 32001, 2]\n",
      "Token: <stop>, ID: [1, 32002, 2]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = tokenizer.get_vocab().keys()\n",
    "new_tokens = [\"<calculator>\", \"</calculator>\", \"<stop>\"]\n",
    "for token in new_tokens:\n",
    "    if token not in vocabulary:\n",
    "        tokenizer.add_tokens(token)\n",
    "        print(f\"Token: {token}, ID: {tokenizer.encode(token)}\")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPER PARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"You are a Personal Assistant named JARVIS. You are installed in Devasheesh Mishra's House and running on his servers. You are integrated in his smart home system. If he is generally talking about something, just talk to him but if he is asking to control the state of a device, you first need to ask the system to give you the functions needed to control the device, then you can do the appropriate function call to control devices. You can also ask user for more information if needed. You can also use tools like 1. calculator (for doing basic mathematical calculations i.e. addition, substraction, multiplication, division) 2. calendar (to find todays date) 3. clock (to find todays day and current time). You can also use Google to search for Information online. Give short responses as much as possible. Be attentive to user commands and inquiries, ensuring a seamless and efficient smart home experience. You are designed to make his life easier and better.\"\n",
    "SYSTEM_PROMPT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_DEVICE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "EPOCHS = 3\n",
    "WARMUP_STEPS = 0\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Zephyr Model Prompt Template:\n",
    "\n",
    "```text\n",
    "<|system|>\n",
    "You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "<|user|>\n",
    "How many helicopters can a human eat in one sitting?</s>\n",
    "<|assistant|>\n",
    "Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        connections.connect(\n",
    "            alias=\"default\",\n",
    "            host=os.getenv(\"MILVUS_HOST\"),\n",
    "            port=19530,\n",
    "            user=os.getenv(\"MILVUS_USER\"),\n",
    "            password=os.getenv(\"MILVUS_PASSWORD\"),\n",
    "        )\n",
    "        db.using_database(\"JARVIS\")\n",
    "        self.collection = Collection(\"calculator\")\n",
    "        self.data_itrator = self.collection.query_iterator(\n",
    "            batch_size=1,\n",
    "            output_fields=[\"conversation\"],\n",
    "            expr=\"conversation_id > 0\",\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.collection.num_entities\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation: dict = self.data_itrator.next()[0]\n",
    "        conversation = self.apply_prompt_template(conversation)\n",
    "        return conversation\n",
    "        # return {\n",
    "        #         'input_ids': conversation['input_ids'].squeeze(),\n",
    "        #         'attention_mask': conversation['attention_mask'].squeeze(),\n",
    "        #     }\n",
    "        # return {\n",
    "        #         'input_ids': torch.tensor(conversation['input_ids'], device=base_model.device),\n",
    "        #         'attention_mask': torch.tensor(conversation['attention_mask'], device=base_model.device)\n",
    "        #     }\n",
    "\n",
    "    def apply_prompt_template(self, conversation: dict):\n",
    "        # add </s> at end of lines which do not end with </s>\n",
    "        # conversation[\"conversation\"][\"data\"] = ''.join([x + '</s>\\n' if not x.strip().endswith(\"</s>\") else x + '\\n' for x in conversation[\"conversation\"][\"data\"].split(\"\\n\")]).strip()\n",
    "\n",
    "        # apply zephyr-7b-beta chat template\n",
    "        prompt = f\"<|system|>\\n{SYSTEM_PROMPT}\\n{conversation[\"conversation\"][\"data\"].replace(\n",
    "            \"USER: \", \"<|user|>\\n\").replace(\"ASSISTANT: \", \"<|assistant|>\\n\")}\"\n",
    "\n",
    "        return prompt.replace('</s>', '<stop>').replace('  ', ' ')\n",
    "        # return tokenizer(prompt, return_tensors=\"pt\", padding=True).to(base_model.device)\n",
    "        # return tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Gradient Accumulation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-accumulation)\n",
    "\n",
    "    The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model’s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU’s memory. In turn, however, the added forward and backward passes can slow down the training a bit.\n",
    "\n",
    "### [Gradient Checkpointing](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing)\n",
    "\n",
    "    Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\n",
    "\n",
    "    Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See this great article explaining the ideas behind gradient checkpointing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.gradient_checkpointing_enable()\n",
    "# base_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@manyi.yim/more-about-loraconfig-from-peft-581cf54643db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 7,248,572,416 || trainable%: 0.09402877710037628\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    # peft_type: str | PeftType = None,\n",
    "    # auto_mapping: dict | None = None,\n",
    "    base_model_name_or_path='zephyr-7b-beta',\n",
    "    # revision: str = None,\n",
    "    # task_type = TaskType.CAUSAL_LM,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # inference_mode: bool = False,\n",
    "    r=16,  # ! 8, 16, 32, 64\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha=32,  # ! 8, 16, 32\n",
    "    lora_dropout=0.05,\n",
    "    # fan_in_fan_out: bool = False,\n",
    "    bias=\"none\",\n",
    "    # modules_to_save: List[str] | None = None,\n",
    "    # init_lora_weights: bool = True,\n",
    "    # layers_to_transform: List[int] | int | None = None,\n",
    "    # layers_pattern: str | None = None\n",
    ")\n",
    "model = get_peft_model(\n",
    "    model, lora_config, adapter_name='jarvis-calculator-v0_1')\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.0.input_layernorm.weight False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.1.input_layernorm.weight False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.2.input_layernorm.weight False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.3.input_layernorm.weight False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.4.input_layernorm.weight False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.5.input_layernorm.weight False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.6.input_layernorm.weight False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.7.input_layernorm.weight False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.8.input_layernorm.weight False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.9.input_layernorm.weight False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.10.input_layernorm.weight False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.11.input_layernorm.weight False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.12.input_layernorm.weight False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.13.input_layernorm.weight False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.14.input_layernorm.weight False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.15.input_layernorm.weight False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.16.input_layernorm.weight False\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.17.input_layernorm.weight False\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.18.input_layernorm.weight False\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.19.input_layernorm.weight False\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.20.input_layernorm.weight False\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.21.input_layernorm.weight False\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.22.input_layernorm.weight False\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.23.input_layernorm.weight False\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.24.input_layernorm.weight False\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.25.input_layernorm.weight False\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.26.input_layernorm.weight False\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.27.input_layernorm.weight False\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.28.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.28.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.28.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.28.input_layernorm.weight False\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.29.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.29.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.29.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.29.input_layernorm.weight False\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.30.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.30.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.30.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.30.input_layernorm.weight False\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.31.self_attn.k_proj.weight False\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.jarvis-calculator-v0_1.weight True\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight False\n",
      "base_model.model.model.layers.31.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.31.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.31.input_layernorm.weight False\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight False\n",
      "base_model.model.model.norm.weight False\n",
      "base_model.model.lm_head.weight False\n"
     ]
    }
   ],
   "source": [
    "for parm in model.named_parameters():\n",
    "    print(parm[0], parm[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MilvusDataset()\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.95 * total_size)\n",
    "test_size = total_size - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = list(train_dataset), list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = [{\"text\": text} for text in train_dataset], [\n",
    "    {\"text\": text} for text in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = datasets.Dataset.from_list(\n",
    "    train_dataset), datasets.Dataset.from_list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(train_dataset[4]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you ran this function, you will need to rerun the cell above to get the train_dataset and test_dataset\n",
    "# def plot_data_lengths(dataset):\n",
    "#     lengths = [len(x['input_ids']) for x in dataset]\n",
    "#     print(len(lengths))\n",
    "\n",
    "#     # Plotting the histogram\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.hist(lengths, bins=40, alpha=0.7, color='blue')\n",
    "#     plt.xlabel('Length of input_ids')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.title('Distribution of Lengths of input_ids')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_data_lengths(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "run_name = \"calculator-lora\"\n",
    "training_args = TrainingArguments(\n",
    "    # Training\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    # gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_DEVICE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    # fp16=True,\n",
    "\n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "\n",
    "    # Logging\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=len(train_dataset) // (BATCH_SIZE_PER_DEVICE * GRADIENT_ACCUMULATION_STEPS * EPOCHS),\n",
    "\n",
    "    # Checkpointing\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=10,\n",
    "    output_dir=_LORA_OUTPUT_PATH,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset['text'],\n",
    "#     eval_dataset=test_dataset['text'],\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72a472bf00946d5b6ebeabd424c8fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0860b281e2a042239a41466b438db64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "***** Running training *****\n",
      "  Num examples = 51\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 153\n",
      "  Number of trainable parameters = 6,815,744\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/nvme/JARVIS/function_call_generator/experiments/wandb/run-20240120_183413-en7ucgw8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devasheeshmishra/huggingface/runs/en7ucgw8' target=\"_blank\">calculator-lora-2024-01-20-18-34</a></strong> to <a href='https://wandb.ai/devasheeshmishra/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devasheeshmishra/huggingface' target=\"_blank\">https://wandb.ai/devasheeshmishra/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devasheeshmishra/huggingface/runs/en7ucgw8' target=\"_blank\">https://wandb.ai/devasheeshmishra/huggingface/runs/en7ucgw8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 02:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.081802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.629400</td>\n",
       "      <td>0.801121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.629400</td>\n",
       "      <td>0.674294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.619689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.598149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.577947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.575065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.573461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.566265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.547697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.604800</td>\n",
       "      <td>0.541774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.562149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.573334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.574897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.576401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=153, training_loss=0.6563566058289771, metrics={'train_runtime': 169.1975, 'train_samples_per_second': 0.904, 'train_steps_per_second': 0.904, 'total_flos': 925757445832704.0, 'train_loss': 0.6563566058289771, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"jarvis-calculator-lora\", name=run_name, config=training_args)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    dataset_text_field='text',\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    max_seq_length=MAX_LENGTH,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_bos_token = False\n",
    "tokenizer.add_eos_token = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "I need help calculating the total cost of buying groceries.\n",
      "<|assistant|>\n",
      "I can help you with that. What are the items you want to buy, along with their respective quantities and prices?  <stop> \n",
      "<|user|>\n",
      "3 dozen eggs at 2 dollars per dozen, 5 pounds of chicken at 6 dollars per pound, 2 bags of rice at 10 dollars per bag, and 4 cans of tomato sauce at 3 dollars per can.\n",
      "<|assistant|>\n",
      "The total cost of your groceries comes out to  <calculator> 3 X 2 + 5 X 6 + 2 X 10 + 4 X 3  <stop> 68  </calculator> sixty-eight dollars.  <stop>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer(train_dataset['text'][1])['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|system|>\n",
    "<|user|>\n",
    "what is 213 plus 324?\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "prompt = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stops: list = []):\n",
    "        StoppingCriteria.__init__(self)\n",
    "        self.stops = stops\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for token in self.stops:\n",
    "            if input_ids[0][-1].cpu().numpy() == token:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "<|user|>\n",
      "what is 213 plus 324?\n",
      "<|assistant|>\n",
      "The sum of 213 and 324 is 213+324  <stop>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    **prompt,\n",
    "    max_length=256,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    stopping_criteria=StoppingCriteriaList([\n",
    "        CustomStoppingCriteria(stops=[tokenizer.encode(\"<stop>\")[0]])\n",
    "    ]),\n",
    ")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.LongTensor([266]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  523, 28766,  6574, 28766, 28767,    13, 28789, 28766,  1838, 28766,\n",
       "         28767,    13, 28737,   927,  1316,  4900,  1077,   272,  3102,  2434,\n",
       "           302,  9737,  5977,  2742,   497, 28723,    13, 28789, 28766,   489,\n",
       "         11143, 28766, 28767,    13, 22099, 28725,  4665,  3084,   528,   395,\n",
       "           272,  2434,   302,  1430,  2515,   304,   272, 15069,   302,  1430,\n",
       "          2515,   368,   947,   298,  3848, 28723,  2287, 32002,   259,    13,\n",
       "         28789, 28766,  1838, 28766, 28767,    13, 28737,  7620, 28705, 28750,\n",
       "           979,  2815,   354, 28705, 28740, 28782,  6869,   386,   274,  1430,\n",
       "         28725, 28705, 28770,   442,  5897,   354, 28705, 28750, 28734,  6869,\n",
       "           386,   274,  1430, 28725,   304, 28705, 28740,  8743,  2238,   354,\n",
       "         28705, 28740, 28734,  6869,   386,   274, 28723,    13, 28789, 28766,\n",
       "           489, 11143, 28766, 28767,    13,  1014,  3102,  2434,   302,  9737,\n",
       "          5977,  2742,   497,   349,   259, 32000,   259, 28750, 28736, 28740,\n",
       "         28782, 28806, 28770, 28736, 28750, 28734, 28806, 28740, 28736, 28740,\n",
       "         28734,   259, 32002,   259, 28740, 28734, 28734, 28705, 32001, 28705,\n",
       "           624,  4682,  6869,   386,   274, 28723, 28705, 32002]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
