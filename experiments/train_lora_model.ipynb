{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TextStreamer, \n",
    "    GenerationConfig, \n",
    "    logging,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import datasets\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _OPENFUNCTIONS_TEST = \"datasets/gorilla_openfunctions/test.jsonl\"\n",
    "# _OPENFUNCTIONS_TRAIN = \"datasets/gorilla_openfunctions/train.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Zephyr Model Prompt Template:\n",
    "```text\n",
    "<|system|>\n",
    "You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "<|user|>\n",
    "How many helicopters can a human eat in one sitting?</s>\n",
    "<|assistant|>\n",
    "Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_json(_OPENFUNCTIONS_TEST, lines=True)\n",
    "# train_data = pd.read_json(_OPENFUNCTIONS_TRAIN, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_types = {\n",
    "#     'question': 'string',\n",
    "#     'function': 'string',\n",
    "#     'model_answer': 'string',\n",
    "# }\n",
    "# test_data = test_data.astype(column_types)\n",
    "# train_data = train_data.astype(column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['Functions'][432]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BASE_MODEL_PATH = Path('../models/zephyr-7b-beta/')\n",
    "_LORA_OUTPUT_PATH = Path('../models/loras/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_tokenizer = AutoTokenizer.from_pretrained(_BASE_MODEL_PATH, use_fast=False)\n",
    "base_model_config = AutoConfig.from_pretrained(_BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_config.torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    _BASE_MODEL_PATH,\n",
    "    config=base_model_config, \n",
    "    device_map='auto', \n",
    "    torch_dtype=base_model_config.torch_dtype,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in base_model.parameters():\n",
    "    # Turning off gradient calculation for base model as we want to train lora, not base model\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_tokenizer.bos_token, base_model_tokenizer.pad_token, base_model_tokenizer.eos_token, base_model_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Gradient Accumulation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-accumulation)\n",
    "    The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model’s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU’s memory. In turn, however, the added forward and backward passes can slow down the training a bit.\n",
    "\n",
    "### [Gradient Checkpointing](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing)\n",
    "    Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\n",
    "\n",
    "    Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See this great article explaining the ideas behind gradient checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.gradient_checkpointing_enable()\n",
    "base_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable(%): {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@manyi.yim/more-about-loraconfig-from-peft-581cf54643db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    # peft_type: str | PeftType = None,\n",
    "    # auto_mapping: dict | None = None,\n",
    "    # base_model_name_or_path: str = None,\n",
    "    # revision: str = None,\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    "    # inference_mode: bool = False,\n",
    "    r = 64, #! 8, 16, 32, 64\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha = 16, #! 8, 16, 32\n",
    "    lora_dropout = 0.1, #! 0.05\n",
    "    # fan_in_fan_out: bool = False,\n",
    "    bias = \"none\",\n",
    "    # modules_to_save: List[str] | None = None,\n",
    "    # init_lora_weights: bool = True,\n",
    "    # layers_to_transform: List[int] | int | None = None,\n",
    "    # layers_pattern: str | None = None\n",
    ")\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainig_parms = TrainingArguments(\n",
    "    output_dir=_LORA_OUTPUT_PATH,\n",
    "    num_train_epochs=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    \n",
    "    logging_steps=25, # Default: 500\n",
    "    # fp16=True,\n",
    "    \n",
    "    save_steps=25,\n",
    "    save_safetensors=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "data = datasets.load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: base_model_tokenizer(samples['quote']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'] = data['train'].select(range(100))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=data['train'],\n",
    "    # eval_dataset=data['validation'],\n",
    "    args=trainig_parms,\n",
    "    tokenizer=base_model_tokenizer,\n",
    "    # callbacks=[],\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(base_model_tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = data['train'][76]['input_ids']\n",
    "prompt = torch.tensor(prompt).unsqueeze(0)\n",
    "outputs = base_model.generate(prompt)\n",
    "base_model_tokenizer.decode(outputs[0], skip_special_tokens=True), data['train'][76]['quote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(_LORA_OUTPUT_PATH)\n",
    "peft_model.config.save_pretrained(_LORA_OUTPUT_PATH)\n",
    "peft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    _LORA_OUTPUT_PATH,\n",
    "    config=base_model_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=base_model_config.torch_dtype,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_tokenizer = AutoTokenizer.from_pretrained(_BASE_MODEL_PATH, use_fast=False)\n",
    "\n",
    "prompt = data['train'][76]['input_ids'][:5]\n",
    "prompt = torch.tensor(prompt).unsqueeze(0)\n",
    "outputs = peft_model.generate(prompt)\n",
    "peft_model_tokenizer.decode(outputs[0], skip_special_tokens=True), data['train'][76]['quote']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
